{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38426068.89260407\n",
      "1 37736730.004088655\n",
      "2 40614810.20402816\n",
      "3 38817352.91636294\n",
      "4 28919301.374682598\n",
      "5 16430353.422094183\n",
      "6 7860014.993222479\n",
      "7 3801198.186288275\n",
      "8 2149572.5336797796\n",
      "9 1450925.6837565466\n",
      "10 1104093.1858348232\n",
      "11 894588.454155062\n",
      "12 747673.3974653035\n",
      "13 635228.5411766414\n",
      "14 545119.638661385\n",
      "15 471195.59278030746\n",
      "16 409734.3233111331\n",
      "17 358087.4239206467\n",
      "18 314367.9685270435\n",
      "19 277138.4417114052\n",
      "20 245281.23546429633\n",
      "21 217867.43386744073\n",
      "22 194169.14297790788\n",
      "23 173557.61035512557\n",
      "24 155583.98399208573\n",
      "25 139844.7499863392\n",
      "26 125988.80251011708\n",
      "27 113752.55585937937\n",
      "28 102910.72995037521\n",
      "29 93286.41387398983\n",
      "30 84726.46159635001\n",
      "31 77088.1813099533\n",
      "32 70259.3228659976\n",
      "33 64143.59515516405\n",
      "34 58645.80564280742\n",
      "35 53695.76907992133\n",
      "36 49226.49322445566\n",
      "37 45185.79663835936\n",
      "38 41529.12625240363\n",
      "39 38214.56030961045\n",
      "40 35200.12307532804\n",
      "41 32455.080081778367\n",
      "42 29954.58664521911\n",
      "43 27672.579548151727\n",
      "44 25586.60184812051\n",
      "45 23678.443834267644\n",
      "46 21930.79843785655\n",
      "47 20328.62123949034\n",
      "48 18856.86244980972\n",
      "49 17503.94299709439\n",
      "50 16262.801097821339\n",
      "51 15120.483763332848\n",
      "52 14067.64854670033\n",
      "53 13096.268847986103\n",
      "54 12199.894212935687\n",
      "55 11371.84489753332\n",
      "56 10605.963085588599\n",
      "57 9897.422779266148\n",
      "58 9240.753556656065\n",
      "59 8632.486888240595\n",
      "60 8068.731869811836\n",
      "61 7544.784632148293\n",
      "62 7058.332690700802\n",
      "63 6606.110465030482\n",
      "64 6185.676484439551\n",
      "65 5794.815889410838\n",
      "66 5430.517625754164\n",
      "67 5091.144505357523\n",
      "68 4774.827794320592\n",
      "69 4479.771892954313\n",
      "70 4204.618855260715\n",
      "71 3947.7072850571826\n",
      "72 3707.9011699884786\n",
      "73 3483.8910564189796\n",
      "74 3274.475419455303\n",
      "75 3078.7716954641965\n",
      "76 2895.362962516661\n",
      "77 2723.706649199026\n",
      "78 2562.9397466407668\n",
      "79 2412.3176609463358\n",
      "80 2271.211791195442\n",
      "81 2138.8737468041886\n",
      "82 2014.8454617407847\n",
      "83 1898.4497456165946\n",
      "84 1789.1630753627855\n",
      "85 1686.5760567880757\n",
      "86 1590.2062415198916\n",
      "87 1499.7671450184089\n",
      "88 1414.6809055759122\n",
      "89 1334.7579820933488\n",
      "90 1259.630216088919\n",
      "91 1188.9585635743545\n",
      "92 1122.428105809759\n",
      "93 1059.7969150888093\n",
      "94 1000.8524773141472\n",
      "95 945.3576683017991\n",
      "96 893.0839484376919\n",
      "97 843.8710772080922\n",
      "98 797.4950910210821\n",
      "99 753.8033914036901\n",
      "100 712.6156219022502\n",
      "101 673.7842384388572\n",
      "102 637.1468660447923\n",
      "103 602.5852878464667\n",
      "104 569.9843557012298\n",
      "105 539.2123934585322\n",
      "106 510.1969107272734\n",
      "107 482.80500885498446\n",
      "108 456.9371308950008\n",
      "109 432.49946131523603\n",
      "110 409.4254191851404\n",
      "111 387.63202613136707\n",
      "112 367.0432442557861\n",
      "113 347.5906141355905\n",
      "114 329.19711585352405\n",
      "115 311.84123888074356\n",
      "116 295.4238460268933\n",
      "117 279.89763446611846\n",
      "118 265.21313090221867\n",
      "119 251.32702079536813\n",
      "120 238.19188054417222\n",
      "121 225.7626280962064\n",
      "122 214.0001108901285\n",
      "123 202.8627471389891\n",
      "124 192.3328815958534\n",
      "125 182.35627244329805\n",
      "126 172.91988745759193\n",
      "127 163.98433515822688\n",
      "128 155.52485966784624\n",
      "129 147.51029672152438\n",
      "130 139.91969859601727\n",
      "131 132.73884840027932\n",
      "132 125.93756881166459\n",
      "133 119.48945124832773\n",
      "134 113.37446372223496\n",
      "135 107.584322217845\n",
      "136 102.09475277083907\n",
      "137 96.89488910702417\n",
      "138 91.96411486170327\n",
      "139 87.29182926442505\n",
      "140 82.86219711561967\n",
      "141 78.66544698733546\n",
      "142 74.68253425832299\n",
      "143 70.90542702978195\n",
      "144 67.32546077911127\n",
      "145 63.92956842271084\n",
      "146 60.70816990498623\n",
      "147 57.65133457547451\n",
      "148 54.752838768764306\n",
      "149 52.00245626010273\n",
      "150 49.39578791299871\n",
      "151 46.92128746308596\n",
      "152 44.57243871959183\n",
      "153 42.342689456953785\n",
      "154 40.22711467006869\n",
      "155 38.218409269364685\n",
      "156 36.312479137568296\n",
      "157 34.50316117339848\n",
      "158 32.785578141060356\n",
      "159 31.155307479711627\n",
      "160 29.607198306313414\n",
      "161 28.137214350233464\n",
      "162 26.741704212574753\n",
      "163 25.416472111972745\n",
      "164 24.15732512056924\n",
      "165 22.96209088520365\n",
      "166 21.826492961971713\n",
      "167 20.748322288404047\n",
      "168 19.724187086044157\n",
      "169 18.751528028658417\n",
      "170 17.827073180825085\n",
      "171 16.949145985118065\n",
      "172 16.114743295360118\n",
      "173 15.322835083034398\n",
      "174 14.569433537067631\n",
      "175 13.853913817939395\n",
      "176 13.174183592635401\n",
      "177 12.529328825700434\n",
      "178 11.915076736737818\n",
      "179 11.331451837564483\n",
      "180 10.776560529852418\n",
      "181 10.249583747470847\n",
      "182 9.748196486851764\n",
      "183 9.272095534535005\n",
      "184 8.819274550025222\n",
      "185 8.389051777659226\n",
      "186 7.979984335253539\n",
      "187 7.591073010757899\n",
      "188 7.22124127915399\n",
      "189 6.8699588182067775\n",
      "190 6.535785738141616\n",
      "191 6.21821821601601\n",
      "192 5.9160929529878725\n",
      "193 5.628973547328678\n",
      "194 5.355828051915932\n",
      "195 5.0963349142943795\n",
      "196 4.849287853624382\n",
      "197 4.614492114643621\n",
      "198 4.391008171716275\n",
      "199 4.178531022773327\n",
      "200 3.976343267712318\n",
      "201 3.784130034520077\n",
      "202 3.6011848923681056\n",
      "203 3.427240989340178\n",
      "204 3.2617809383964222\n",
      "205 3.104422143358242\n",
      "206 2.954619188498528\n",
      "207 2.812158624266192\n",
      "208 2.6765781089712597\n",
      "209 2.5475964620065747\n",
      "210 2.4248963408829076\n",
      "211 2.3081387447773265\n",
      "212 2.197046701111997\n",
      "213 2.091348008260146\n",
      "214 1.9908616101000374\n",
      "215 1.8951417853206425\n",
      "216 1.8040838398545898\n",
      "217 1.7174081251944868\n",
      "218 1.6349740854716692\n",
      "219 1.5564595790108697\n",
      "220 1.481774906802105\n",
      "221 1.4106862331285994\n",
      "222 1.3430759473910188\n",
      "223 1.2787012920449978\n",
      "224 1.217493216330738\n",
      "225 1.1591855008975416\n",
      "226 1.1036608616287942\n",
      "227 1.0508283000436995\n",
      "228 1.0005858650265942\n",
      "229 0.9527602074103981\n",
      "230 0.9071754991895886\n",
      "231 0.8638177035139889\n",
      "232 0.822519455881274\n",
      "233 0.7832238628864955\n",
      "234 0.7458294125769072\n",
      "235 0.7102129529400449\n",
      "236 0.6763048151510418\n",
      "237 0.6440157448450506\n",
      "238 0.6132994522628034\n",
      "239 0.5840366241685213\n",
      "240 0.5561937168420008\n",
      "241 0.529672795060576\n",
      "242 0.5044272672157856\n",
      "243 0.4803914229922268\n",
      "244 0.457516299761346\n",
      "245 0.43573347162378157\n",
      "246 0.4149809965667783\n",
      "247 0.395231498250245\n",
      "248 0.37642639073706247\n",
      "249 0.35851681869949675\n",
      "250 0.34146061972393416\n",
      "251 0.3252215925592111\n",
      "252 0.3097609036383996\n",
      "253 0.2950338062059484\n",
      "254 0.28102445048438107\n",
      "255 0.2676745524207905\n",
      "256 0.25496183227371605\n",
      "257 0.2428564602956192\n",
      "258 0.23132497099252897\n",
      "259 0.22034483504890628\n",
      "260 0.20988419472341283\n",
      "261 0.19992683605996137\n",
      "262 0.19044021175165038\n",
      "263 0.1814069236463955\n",
      "264 0.17281153103076957\n",
      "265 0.16461829069645892\n",
      "266 0.1568165516791287\n",
      "267 0.14938398853165338\n",
      "268 0.14230552541610983\n",
      "269 0.13556422003710905\n",
      "270 0.1291437510035216\n",
      "271 0.12302925977637444\n",
      "272 0.1172030603153433\n",
      "273 0.11165664898530517\n",
      "274 0.10637544328951667\n",
      "275 0.10134139820446925\n",
      "276 0.09654691593655618\n",
      "277 0.09197908456207911\n",
      "278 0.08762868025050796\n",
      "279 0.08348457150988531\n",
      "280 0.07953767616867016\n",
      "281 0.07577902728756647\n",
      "282 0.07219697899167418\n",
      "283 0.06878597696794438\n",
      "284 0.065537511706318\n",
      "285 0.062443347140739944\n",
      "286 0.059498171775037204\n",
      "287 0.0566879863929695\n",
      "288 0.05401116080785949\n",
      "289 0.051462587126559003\n",
      "290 0.04903366355225747\n",
      "291 0.0467197898977764\n",
      "292 0.04451496640529682\n",
      "293 0.04241445078603424\n",
      "294 0.04041434559002227\n",
      "295 0.03850905673637344\n",
      "296 0.03669343394787731\n",
      "297 0.034963621669203276\n",
      "298 0.03331499165889825\n",
      "299 0.03174478972113573\n",
      "300 0.03024830636136803\n",
      "301 0.0288226815582276\n",
      "302 0.02746465348670809\n",
      "303 0.026170363983318515\n",
      "304 0.02493783246689439\n",
      "305 0.023764394408704922\n",
      "306 0.022645193765163305\n",
      "307 0.021578981166265974\n",
      "308 0.020562959350126916\n",
      "309 0.019594687686862535\n",
      "310 0.018672522082762273\n",
      "311 0.017793700254774295\n",
      "312 0.016956449333973153\n",
      "313 0.016158744596095977\n",
      "314 0.015398527194285958\n",
      "315 0.014674256570709478\n",
      "316 0.013984532524970113\n",
      "317 0.013326814728193156\n",
      "318 0.012700100926611132\n",
      "319 0.012102977090633488\n",
      "320 0.011534082547799748\n",
      "321 0.010992042514513467\n",
      "322 0.010475368518532873\n",
      "323 0.009983003850883618\n",
      "324 0.009513965381624072\n",
      "325 0.009066980758580576\n",
      "326 0.008641211955203641\n",
      "327 0.008235541600402395\n",
      "328 0.007848783920262656\n",
      "329 0.0074802870089256054\n",
      "330 0.007129110445994985\n",
      "331 0.006794321657419547\n",
      "332 0.006475320932601611\n",
      "333 0.006171370905871079\n",
      "334 0.005881682197910182\n",
      "335 0.0056056603984720775\n",
      "336 0.00534262401887294\n",
      "337 0.005092111436351257\n",
      "338 0.004853210921512845\n",
      "339 0.004625573852542336\n",
      "340 0.004408560199045218\n",
      "341 0.004201777422494411\n",
      "342 0.004004781760429515\n",
      "343 0.003816953020828737\n",
      "344 0.0036379566034708717\n",
      "345 0.0034674037098563933\n",
      "346 0.0033048446851529373\n",
      "347 0.0031499336006646475\n",
      "348 0.0030023978629941895\n",
      "349 0.0028616903596854365\n",
      "350 0.0027276065070159575\n",
      "351 0.002599919475393288\n",
      "352 0.0024782200782529663\n",
      "353 0.0023621090330436098\n",
      "354 0.002251468252986159\n",
      "355 0.0021460229132971628\n",
      "356 0.002045520568870335\n",
      "357 0.0019497261281890322\n",
      "358 0.0018584444012378592\n",
      "359 0.0017714667003555186\n",
      "360 0.0016885157814657092\n",
      "361 0.001609489403800059\n",
      "362 0.0015341479449980154\n",
      "363 0.001462334548266801\n",
      "364 0.001393909178319897\n",
      "365 0.001328670968281289\n",
      "366 0.00126649335193665\n",
      "367 0.0012072326873954\n",
      "368 0.0011507520727170165\n",
      "369 0.0010969140001979337\n",
      "370 0.0010456258913028693\n",
      "371 0.0009967341030633937\n",
      "372 0.0009501218621412647\n",
      "373 0.000905686028258858\n",
      "374 0.0008633303046408508\n",
      "375 0.0008229730529547806\n",
      "376 0.0007844941678923933\n",
      "377 0.0007478122214911947\n",
      "378 0.0007128637364901126\n",
      "379 0.0006795411976616311\n",
      "380 0.0006477820016687775\n",
      "381 0.0006175227384378973\n",
      "382 0.0005886640878372395\n",
      "383 0.00056115558802655\n",
      "384 0.0005349339089266898\n",
      "385 0.0005099471959037675\n",
      "386 0.0004861199720677821\n",
      "387 0.0004634063703795035\n",
      "388 0.00044176373286878624\n",
      "389 0.00042113482091614565\n",
      "390 0.0004014624583053096\n",
      "391 0.00038271260347362134\n",
      "392 0.0003648541153082652\n",
      "393 0.00034781690160558325\n",
      "394 0.0003315785808839579\n",
      "395 0.0003160981797819979\n",
      "396 0.0003013412621504783\n",
      "397 0.00028727514551249716\n",
      "398 0.00027386349962508756\n",
      "399 0.0002610818828954406\n",
      "400 0.0002488991258164002\n",
      "401 0.00023728096383523176\n",
      "402 0.00022620650484708968\n",
      "403 0.000215657988728959\n",
      "404 0.0002055961253060706\n",
      "405 0.00019600380126020786\n",
      "406 0.00018686228065887221\n",
      "407 0.00017814519107336852\n",
      "408 0.00016983643709029484\n",
      "409 0.00016191416448554217\n",
      "410 0.00015436144360061059\n",
      "411 0.00014716226550075674\n",
      "412 0.00014029947025736138\n",
      "413 0.00013375726566010654\n",
      "414 0.00012752489156161596\n",
      "415 0.00012157972614422875\n",
      "416 0.0001159111930961945\n",
      "417 0.00011050755814887218\n",
      "418 0.00010535628098914386\n",
      "419 0.00010044490647978772\n",
      "420 9.576444957974615e-05\n",
      "421 9.130083206071587e-05\n",
      "422 8.704537633370834e-05\n",
      "423 8.298939637841813e-05\n",
      "424 7.912349248008558e-05\n",
      "425 7.544370691446837e-05\n",
      "426 7.193077481283109e-05\n",
      "427 6.857955960185742e-05\n",
      "428 6.538464943960845e-05\n",
      "429 6.234093608812917e-05\n",
      "430 5.9436654239585404e-05\n",
      "431 5.6667900034183045e-05\n",
      "432 5.4028845212766514e-05\n",
      "433 5.1512549021623504e-05\n",
      "434 4.911389577252695e-05\n",
      "435 4.682671877995856e-05\n",
      "436 4.464714949013775e-05\n",
      "437 4.2568800474783907e-05\n",
      "438 4.058684491104955e-05\n",
      "439 3.869704846757889e-05\n",
      "440 3.689570202965361e-05\n",
      "441 3.517886438078273e-05\n",
      "442 3.354125593163434e-05\n",
      "443 3.197992184113389e-05\n",
      "444 3.049144499325991e-05\n",
      "445 2.907214245510161e-05\n",
      "446 2.7719301616272143e-05\n",
      "447 2.6429772617408806e-05\n",
      "448 2.520045125693738e-05\n",
      "449 2.4027824173615563e-05\n",
      "450 2.291003792427815e-05\n",
      "451 2.1844252295351402e-05\n",
      "452 2.0828083935456287e-05\n",
      "453 1.985911598017335e-05\n",
      "454 1.8935247905042178e-05\n",
      "455 1.8054693845442168e-05\n",
      "456 1.7214908568045284e-05\n",
      "457 1.641415762571034e-05\n",
      "458 1.5650856907625313e-05\n",
      "459 1.4923498568103451e-05\n",
      "460 1.4229510002085667e-05\n",
      "461 1.3568011278763632e-05\n",
      "462 1.2937168048959044e-05\n",
      "463 1.2335693120726053e-05\n",
      "464 1.1762238564604884e-05\n",
      "465 1.1215438383979454e-05\n",
      "466 1.0694008183963852e-05\n",
      "467 1.0196930613240474e-05\n",
      "468 9.72297985928245e-06\n",
      "469 9.271052402867009e-06\n",
      "470 8.840405854492554e-06\n",
      "471 8.429609773001291e-06\n",
      "472 8.037810891930529e-06\n",
      "473 7.664283456094261e-06\n",
      "474 7.308111315759551e-06\n",
      "475 6.968639181550706e-06\n",
      "476 6.644911585908477e-06\n",
      "477 6.3361640502270695e-06\n",
      "478 6.041778978939657e-06\n",
      "479 5.761117394014974e-06\n",
      "480 5.493466183873152e-06\n",
      "481 5.2383445831314e-06\n",
      "482 4.99517850471496e-06\n",
      "483 4.763144556500623e-06\n",
      "484 4.541889512907597e-06\n",
      "485 4.330967193860469e-06\n",
      "486 4.129854218553014e-06\n",
      "487 3.938098203215852e-06\n",
      "488 3.7552292010748667e-06\n",
      "489 3.580876129600274e-06\n",
      "490 3.4145975352177196e-06\n",
      "491 3.256058696785083e-06\n",
      "492 3.104894595767149e-06\n",
      "493 2.9608350938975625e-06\n",
      "494 2.823397071602783e-06\n",
      "495 2.69236896201919e-06\n",
      "496 2.5674308839576445e-06\n",
      "497 2.4482613559759272e-06\n",
      "498 2.3346358595130722e-06\n",
      "499 2.2262865723124444e-06\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 458.9590148925781\n",
      "199 2.361753225326538\n",
      "299 0.019351312890648842\n",
      "399 0.00038840548950247467\n",
      "499 5.0761140300892293e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 418.6033935546875\n",
      "199 0.9524595737457275\n",
      "299 0.004332978744059801\n",
      "399 0.0001249325869139284\n",
      "499 2.5847195502137765e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward propagation\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Use autograd to compute the backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent.\n",
    "    # Wrap in torch.no_grad() because we don't need to track this\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 463.19317626953125\n",
      "199 1.6042590141296387\n",
      "299 0.01030062697827816\n",
      "399 0.000256611208897084\n",
      "499 4.611706390278414e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    \n",
    "    # relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    \n",
    "    # y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.5310652256011963\n",
      "199 0.04884731397032738\n",
      "299 0.0021178696770220995\n",
      "399 0.00013431801926344633\n",
      "499 1.0115407349076122e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 38.043296813964844\n",
      "199 0.3880680501461029\n",
      "299 0.002169638639315963\n",
      "399 2.6844786589208525e-06\n",
      "499 5.15915310472792e-10\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.9346767663955688\n",
      "199 0.037693046033382416\n",
      "299 0.001441936125047505\n",
      "399 7.576794450869784e-05\n",
      "499 4.809468464372912e-06\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0,3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 39.67962646484375\n",
      "199 5.068668365478516\n",
      "299 29.177148818969727\n",
      "399 80.4691390991211\n",
      "499 1.0059314966201782\n"
     ]
    }
   ],
   "source": [
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
