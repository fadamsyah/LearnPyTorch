{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from resnet import ResNet18, ResNet34\n",
    "from utils import OptimOneCycleLR, LabelSmoothingCrossEntropy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25, hue=0.1, saturation=0.1),\n",
    "    transforms.RandomRotation(degrees=30.),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5070754 , 0.48655024, 0.44091907), (0.26733398, 0.25643876, 0.2761503)),\n",
    "    transforms.RandomErasing(p=0.75, scale=(0.02, 0.1), ratio=(0.2, 5), value=0)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5070754 , 0.48655024, 0.44091907), (0.26733398, 0.25643876, 0.2761503))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='../data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='../data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = trainset.__len__()\n",
    "num_test = testset.__len__()\n",
    "bs_train = 1024\n",
    "bs_test = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=bs_train, shuffle=True, num_workers=4, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=bs_test, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(64, 100)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = LabelSmoothingCrossEntropy()\n",
    "\n",
    "# SGD\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.08, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
    "scheduler = OptimOneCycleLR(optimizer, 0.1, 1., 0.001,\n",
    "                            epochs, len(trainloader), 0.25, 0.25, 0., 0., 'linear')\n",
    "\n",
    "# AdamW\n",
    "# optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, betas=[0.9, 0.999], eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
    "# scheduler = OptimOneCycleLR(optimizer, 1e-3, 1e-2, 1e-5,\n",
    "#                             epochs, len(trainloader), 0.1, 0.5, 0.2, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training for 100 epoch\n",
      "Epoch: 1 || train_loss: 4.22012 || train_acc : 0.06078 || test_loss: 4.49792 || test_acc : 0.07590 \n",
      "Epoch: 2 || train_loss: 3.80961 || train_acc : 0.11108 || test_loss: 3.54655 || test_acc : 0.14840 \n",
      "Epoch: 3 || train_loss: 3.60165 || train_acc : 0.14504 || test_loss: 3.34925 || test_acc : 0.19080 \n",
      "Epoch: 4 || train_loss: 3.42955 || train_acc : 0.17522 || test_loss: 3.18163 || test_acc : 0.22350 \n",
      "Epoch: 5 || train_loss: 3.27281 || train_acc : 0.20198 || test_loss: 3.00147 || test_acc : 0.25230 \n",
      "Epoch: 6 || train_loss: 3.10431 || train_acc : 0.23290 || test_loss: 2.82128 || test_acc : 0.29650 \n",
      "Epoch: 7 || train_loss: 2.95424 || train_acc : 0.25932 || test_loss: 2.78559 || test_acc : 0.31370 \n",
      "Epoch: 8 || train_loss: 2.82819 || train_acc : 0.28712 || test_loss: 2.49408 || test_acc : 0.36090 \n",
      "Epoch: 9 || train_loss: 2.69828 || train_acc : 0.31514 || test_loss: 2.49864 || test_acc : 0.37260 \n",
      "Epoch: 10 || train_loss: 2.59103 || train_acc : 0.33710 || test_loss: 2.35106 || test_acc : 0.39870 \n",
      "Epoch: 11 || train_loss: 2.49194 || train_acc : 0.35274 || test_loss: 2.25679 || test_acc : 0.40890 \n",
      "Epoch: 12 || train_loss: 2.39962 || train_acc : 0.37394 || test_loss: 2.23101 || test_acc : 0.41770 \n",
      "Epoch: 13 || train_loss: 2.31959 || train_acc : 0.39234 || test_loss: 2.07898 || test_acc : 0.44950 \n",
      "Epoch: 14 || train_loss: 2.23860 || train_acc : 0.40860 || test_loss: 1.92301 || test_acc : 0.47940 \n",
      "Epoch: 15 || train_loss: 2.17164 || train_acc : 0.42440 || test_loss: 1.93318 || test_acc : 0.48390 \n",
      "Epoch: 16 || train_loss: 2.10583 || train_acc : 0.44000 || test_loss: 1.88454 || test_acc : 0.49800 \n",
      "Epoch: 17 || train_loss: 2.05599 || train_acc : 0.44952 || test_loss: 1.83781 || test_acc : 0.50800 \n",
      "Epoch: 18 || train_loss: 1.99762 || train_acc : 0.46178 || test_loss: 1.75148 || test_acc : 0.52910 \n",
      "Epoch: 19 || train_loss: 1.95883 || train_acc : 0.47126 || test_loss: 1.71072 || test_acc : 0.53620 \n",
      "Epoch: 20 || train_loss: 1.89812 || train_acc : 0.48412 || test_loss: 1.66055 || test_acc : 0.54740 \n",
      "Epoch: 21 || train_loss: 1.86172 || train_acc : 0.49316 || test_loss: 1.62782 || test_acc : 0.55630 \n",
      "Epoch: 22 || train_loss: 1.81989 || train_acc : 0.50192 || test_loss: 1.58831 || test_acc : 0.56150 \n",
      "Epoch: 23 || train_loss: 1.78924 || train_acc : 0.51006 || test_loss: 1.55311 || test_acc : 0.58040 \n",
      "Epoch: 24 || train_loss: 1.74985 || train_acc : 0.51906 || test_loss: 1.65979 || test_acc : 0.55240 \n",
      "Epoch: 25 || train_loss: 1.72967 || train_acc : 0.52542 || test_loss: 1.50778 || test_acc : 0.59000 \n",
      "Epoch: 26 || train_loss: 1.68423 || train_acc : 0.53374 || test_loss: 1.50548 || test_acc : 0.58650 \n",
      "Epoch: 27 || train_loss: 1.63814 || train_acc : 0.54578 || test_loss: 1.44924 || test_acc : 0.60250 \n",
      "Epoch: 28 || train_loss: 1.60899 || train_acc : 0.55162 || test_loss: 1.38913 || test_acc : 0.61460 \n",
      "Epoch: 29 || train_loss: 1.56507 || train_acc : 0.56484 || test_loss: 1.38408 || test_acc : 0.61910 \n",
      "Epoch: 30 || train_loss: 1.53534 || train_acc : 0.57124 || test_loss: 1.37878 || test_acc : 0.61920 \n",
      "Epoch: 31 || train_loss: 1.50660 || train_acc : 0.57738 || test_loss: 1.37790 || test_acc : 0.62210 \n",
      "Epoch: 32 || train_loss: 1.47652 || train_acc : 0.58388 || test_loss: 1.29144 || test_acc : 0.63940 \n",
      "Epoch: 33 || train_loss: 1.44069 || train_acc : 0.59548 || test_loss: 1.27744 || test_acc : 0.64100 \n",
      "Epoch: 34 || train_loss: 1.41044 || train_acc : 0.59936 || test_loss: 1.30338 || test_acc : 0.64020 \n",
      "Epoch: 35 || train_loss: 1.38319 || train_acc : 0.60750 || test_loss: 1.27474 || test_acc : 0.64600 \n",
      "Epoch: 36 || train_loss: 1.34621 || train_acc : 0.61534 || test_loss: 1.27632 || test_acc : 0.64810 \n",
      "Epoch: 37 || train_loss: 1.33157 || train_acc : 0.61926 || test_loss: 1.28007 || test_acc : 0.64480 \n",
      "Epoch: 38 || train_loss: 1.28861 || train_acc : 0.63126 || test_loss: 1.25367 || test_acc : 0.65650 \n",
      "Epoch: 39 || train_loss: 1.26896 || train_acc : 0.63518 || test_loss: 1.22158 || test_acc : 0.66090 \n",
      "Epoch: 40 || train_loss: 1.24467 || train_acc : 0.64308 || test_loss: 1.25995 || test_acc : 0.65910 \n",
      "Epoch: 41 || train_loss: 1.20919 || train_acc : 0.65154 || test_loss: 1.19120 || test_acc : 0.67090 \n",
      "Epoch: 42 || train_loss: 1.18173 || train_acc : 0.65732 || test_loss: 1.18529 || test_acc : 0.67540 \n",
      "Epoch: 43 || train_loss: 1.15461 || train_acc : 0.66520 || test_loss: 1.16763 || test_acc : 0.68000 \n",
      "Epoch: 44 || train_loss: 1.12420 || train_acc : 0.67078 || test_loss: 1.18936 || test_acc : 0.67730 \n",
      "Epoch: 45 || train_loss: 1.09431 || train_acc : 0.68050 || test_loss: 1.19553 || test_acc : 0.68120 \n",
      "Epoch: 46 || train_loss: 1.06852 || train_acc : 0.68774 || test_loss: 1.15190 || test_acc : 0.68390 \n",
      "Epoch: 47 || train_loss: 1.02856 || train_acc : 0.69602 || test_loss: 1.15517 || test_acc : 0.68780 \n",
      "Epoch: 48 || train_loss: 1.00125 || train_acc : 0.70374 || test_loss: 1.13319 || test_acc : 0.69520 \n",
      "Epoch: 49 || train_loss: 0.96000 || train_acc : 0.71494 || test_loss: 1.12591 || test_acc : 0.69700 \n",
      "Epoch: 50 || train_loss: 0.92719 || train_acc : 0.72354 || test_loss: 1.11364 || test_acc : 0.70410 \n",
      "Epoch: 51 || train_loss: 0.89208 || train_acc : 0.73450 || test_loss: 1.12006 || test_acc : 0.70310 \n",
      "Epoch: 52 || train_loss: 0.86939 || train_acc : 0.73922 || test_loss: 1.12670 || test_acc : 0.70150 \n",
      "Epoch: 53 || train_loss: 0.86024 || train_acc : 0.74164 || test_loss: 1.12446 || test_acc : 0.71010 \n",
      "Epoch: 54 || train_loss: 0.84067 || train_acc : 0.74732 || test_loss: 1.12278 || test_acc : 0.70980 \n",
      "Epoch: 55 || train_loss: 0.83197 || train_acc : 0.74702 || test_loss: 1.12045 || test_acc : 0.70940 \n",
      "Epoch: 56 || train_loss: 0.81248 || train_acc : 0.75292 || test_loss: 1.14383 || test_acc : 0.70630 \n",
      "Epoch: 57 || train_loss: 0.80419 || train_acc : 0.75550 || test_loss: 1.13763 || test_acc : 0.70730 \n",
      "Epoch: 58 || train_loss: 0.78671 || train_acc : 0.76136 || test_loss: 1.13515 || test_acc : 0.70850 \n",
      "Epoch: 59 || train_loss: 0.78129 || train_acc : 0.76338 || test_loss: 1.12771 || test_acc : 0.70620 \n",
      "Epoch: 60 || train_loss: 0.76965 || train_acc : 0.76560 || test_loss: 1.13770 || test_acc : 0.70660 \n",
      "Epoch: 61 || train_loss: 0.75726 || train_acc : 0.76982 || test_loss: 1.14017 || test_acc : 0.70720 \n",
      "Epoch: 62 || train_loss: 0.73716 || train_acc : 0.77506 || test_loss: 1.15811 || test_acc : 0.70360 \n",
      "Epoch: 63 || train_loss: 0.72844 || train_acc : 0.77738 || test_loss: 1.13513 || test_acc : 0.71090 \n",
      "Epoch: 64 || train_loss: 0.71670 || train_acc : 0.78118 || test_loss: 1.15561 || test_acc : 0.71120 \n",
      "Epoch: 65 || train_loss: 0.69626 || train_acc : 0.78558 || test_loss: 1.15992 || test_acc : 0.71100 \n",
      "Epoch: 66 || train_loss: 0.69992 || train_acc : 0.78470 || test_loss: 1.16409 || test_acc : 0.70890 \n",
      "Epoch: 67 || train_loss: 0.69367 || train_acc : 0.78708 || test_loss: 1.17101 || test_acc : 0.71240 \n",
      "Epoch: 68 || train_loss: 0.66529 || train_acc : 0.79382 || test_loss: 1.16163 || test_acc : 0.71260 \n",
      "Epoch: 69 || train_loss: 0.65407 || train_acc : 0.79458 || test_loss: 1.17547 || test_acc : 0.71260 \n",
      "Epoch: 70 || train_loss: 0.64611 || train_acc : 0.80136 || test_loss: 1.19246 || test_acc : 0.70700 \n",
      "Epoch: 71 || train_loss: 0.63866 || train_acc : 0.80144 || test_loss: 1.18949 || test_acc : 0.70860 \n",
      "Epoch: 72 || train_loss: 0.62332 || train_acc : 0.80554 || test_loss: 1.17923 || test_acc : 0.71540 \n",
      "Epoch: 73 || train_loss: 0.62007 || train_acc : 0.80608 || test_loss: 1.20439 || test_acc : 0.71810 \n",
      "Epoch: 74 || train_loss: 0.59676 || train_acc : 0.81312 || test_loss: 1.20745 || test_acc : 0.71360 \n",
      "Epoch: 75 || train_loss: 0.59599 || train_acc : 0.81388 || test_loss: 1.21389 || test_acc : 0.71110 \n",
      "Epoch: 76 || train_loss: 0.58050 || train_acc : 0.81702 || test_loss: 1.21592 || test_acc : 0.71060 \n",
      "Epoch: 77 || train_loss: 0.57843 || train_acc : 0.81854 || test_loss: 1.20025 || test_acc : 0.71630 \n",
      "Epoch: 78 || train_loss: 0.56436 || train_acc : 0.82330 || test_loss: 1.20909 || test_acc : 0.71600 \n",
      "Epoch: 79 || train_loss: 0.54996 || train_acc : 0.82700 || test_loss: 1.20758 || test_acc : 0.71910 \n",
      "Epoch: 80 || train_loss: 0.54747 || train_acc : 0.82642 || test_loss: 1.22592 || test_acc : 0.71590 \n",
      "Epoch: 81 || train_loss: 0.53462 || train_acc : 0.83304 || test_loss: 1.22532 || test_acc : 0.71350 \n",
      "Epoch: 82 || train_loss: 0.52511 || train_acc : 0.83450 || test_loss: 1.22971 || test_acc : 0.71370 \n",
      "Epoch: 83 || train_loss: 0.51002 || train_acc : 0.83828 || test_loss: 1.23133 || test_acc : 0.71980 \n",
      "Epoch: 84 || train_loss: 0.49788 || train_acc : 0.84216 || test_loss: 1.24173 || test_acc : 0.71420 \n",
      "Epoch: 85 || train_loss: 0.49334 || train_acc : 0.84280 || test_loss: 1.24615 || test_acc : 0.71940 \n",
      "Epoch: 86 || train_loss: 0.48447 || train_acc : 0.84450 || test_loss: 1.23514 || test_acc : 0.71870 \n",
      "Epoch: 87 || train_loss: 0.46559 || train_acc : 0.85108 || test_loss: 1.24225 || test_acc : 0.72030 \n",
      "Epoch: 88 || train_loss: 0.46465 || train_acc : 0.85146 || test_loss: 1.25531 || test_acc : 0.71830 \n",
      "Epoch: 89 || train_loss: 0.45087 || train_acc : 0.85524 || test_loss: 1.25373 || test_acc : 0.72210 \n",
      "Epoch: 90 || train_loss: 0.44518 || train_acc : 0.85870 || test_loss: 1.26472 || test_acc : 0.72040 \n",
      "Epoch: 91 || train_loss: 0.43659 || train_acc : 0.86104 || test_loss: 1.26221 || test_acc : 0.71990 \n",
      "Epoch: 93 || train_loss: 0.42365 || train_acc : 0.86332 || test_loss: 1.27225 || test_acc : 0.72020 \n",
      "Epoch: 94 || train_loss: 0.41231 || train_acc : 0.86710 || test_loss: 1.26459 || test_acc : 0.72360 \n",
      "Epoch: 95 || train_loss: 0.41190 || train_acc : 0.86900 || test_loss: 1.26597 || test_acc : 0.72100 \n",
      "Epoch: 96 || train_loss: 0.40014 || train_acc : 0.87198 || test_loss: 1.26614 || test_acc : 0.72150 \n",
      "Epoch: 97 || train_loss: 0.39564 || train_acc : 0.87092 || test_loss: 1.26552 || test_acc : 0.72460 \n",
      "Epoch: 98 || train_loss: 0.38598 || train_acc : 0.87536 || test_loss: 1.27031 || test_acc : 0.72670 \n",
      "Epoch: 99 || train_loss: 0.38732 || train_acc : 0.87662 || test_loss: 1.26162 || test_acc : 0.72710 \n",
      "Epoch: 100 || train_loss: 0.37755 || train_acc : 0.87920 || test_loss: 1.26255 || test_acc : 0.72710 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Begin Training for {epochs} epoch\")\n",
    "test_best_acc = 1000.\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    net.train()\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        pred = net(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        preds = torch.argmax(pred, dim=1)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (preds == y).sum().item()\n",
    "        train_total += y.size(0)\n",
    "        \n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    net.eval()\n",
    "    for x, y in testloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = net(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            preds = torch.argmax(pred, dim=1)\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (preds == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "\n",
    "    print(f'Epoch: {epoch} || train_loss: {train_loss / ((num_train - 1) // bs_train + 1):.5f} || train_acc : {train_correct / train_total:.5f} || test_loss: {test_loss / ((num_test - 1) // bs_test + 1):.5f} || test_acc : {test_correct / test_total:.5f} ')\n",
    "    \n",
    "    if test_best_acc > test_correct / test_total:\n",
    "        torch.save(net.state_dict(), 'models/CIFAR100/ResNet18/model.pt')\n",
    "        \n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss / ((num_train - 1) // bs_train + 1),\n",
    "            }, \"models/CIFAR100/ResNet18/last_checkpoints.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(trainloader))\n",
    "# x, y = x.to(device), y.to(device)\n",
    "\n",
    "# epochs = 100\n",
    "# optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, betas=[0.9, 0.999], eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
    "# scheduler = OptimOneCycleLR(optimizer, 1e-3, 1e-2, 1e-5,\n",
    "#                             epochs, 1, 0.2, 0.4)\n",
    "\n",
    "# print(f\"Begin Training for {epochs} epoch\")\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train_loss = 0\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "    \n",
    "#     net.train()\n",
    "        \n",
    "#     pred = net(x)\n",
    "#     loss = criterion(pred, y)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     preds = torch.argmax(pred, dim=1)\n",
    "#     train_loss += loss.item()\n",
    "#     train_correct += (preds == y).sum().item()\n",
    "#     train_total += y.size(0)\n",
    "\n",
    "#     print(f'Epoch: {epoch} || train_loss: {train_loss / ((num_train - 1) // bs_train + 1):.5f} || train_acc : {train_correct / train_total:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
